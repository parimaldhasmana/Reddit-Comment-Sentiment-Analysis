
	
## **Import Libraries**

```{r}

library(tidyverse)
library(tibble)
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(tidytext)
library(RColorBrewer)
library(reshape2)
library(wordcloud)
library(igraph)
library(widyr)
library(ggraph)
library(ngram)
library(wordcloud2)
library(stringr)
library(ggplot2)
library(tm)
library(wordcloud)
library(knitr)
library(kableExtra)
library(tidytext)
library(tidyverse)
library(tibble)
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(tidytext)
library(RColorBrewer)
library(reshape2)
library(wordcloud)
library(igraph)
library(widyr)
library(ggraph)
library(ngram)
library(wordcloud2)
library(stringr)
library(ggplot2)
library(tm)
library(wordcloud)
library(knitr)
library(kableExtra)
library(data.table)
library(lubridate)

#install.packages("wordcloud")
library(wordcloud)
```	

### **Set Working Directory**
```{r}

setwd("C:\\CIS8392\\reddit")
getwd()
```

# **Reading the data file**
```{r}

file = "worldnews_test3.csv"
df <- fread(file)

```	

### **Sample Data of Reddit dataset **
```{r}
row1 <- head(df, 1)
row1
```

# **Data Summary**

### **1. Total number of rows and columns in the dataset**

```{r }
nrow(df)
ncol(df)
```

### **2. Summary of the datatypes of columns in dataset**
```{r }
str(df)
```

# **Basic Data Cleaning**

### **1. Dropping Link, domain, URL and structure as these columns does not play any role in analyzing comments and further predicting upvotes**

```{r}
df$link <- NULL
df$domain <- NULL
df$URL <- NULL
df$structure <- NULL
```

### **Format time**
```{r}
df$post_date <- dmy(df$post_date)
```

### **Creating seperate columns for POST's Year, Months and Date which will be used later in visualizations**

```{r}
df$post_day <- wday(df$post_date,label = T)

df$post_month <- month(df$post_date,abbr = T,label = T)

df$post_year <- year(df$post_date)

(head(df,1))
```

### **Converting upvote_prop to percentage by multiplying the whole column by 100**
```{r}

df$upvote_prop <- df$upvote_prop*100
```

### **Changing the upvote_prop to a more descriptive name like "upvote_pct"**

```{r}
names(df)[names(df) == 'upvote_prop'] <- 'upvote_pct'

```

### **Let's check for any Null values**
```{r}

colSums(is.na(df))
```


### We don't have any NULL values but we have inappropriate data in comment column i.e [deleted] which is not serving any purpose, let us delete those columns 
```{r}

df <- df[df$comment != "[deleted]"]
#(head(df,5))
```

## **Summary of Dataset**
```{r}
summary(df)
```

# **Data Visualization**

### **1. Post by Year**
```{r}

ggplot(df[,.N,by=post_year],aes(x = post_year,y = N,fill=N, label=round(N,2)))+
  geom_bar(stat = "identity")+labs(title="Posts by year",subtitle="Number of Posts")+xlab("Year")+ylab(NULL)+geom_text(size=5, vjust=1, color="white")

```


### **Inference** -
It can be seen, the highest number of posts are in the year 2020 followed by 2016. In contrast to 2013 which has seen the lowest number of posts

### **2. Post by Month**
```{r}

ggplot(df[,.N,by=post_month],aes(x = post_month,y = N,fill=N, label=round(N,2)))+
  geom_bar(stat = "identity")+labs(title="Posts by month",subtitle="Number of Posts")+xlab("Month")+ylab(NULL)+geom_text(size=5, vjust=1, color="white")

```

### **Inference -**
As is evident, January month dominates the number of posts.Its interesting to see the pattern i.e Number of posts is maintained ~950 for the months of February, April, May and June while for the rest of the months its ~500. This will be really useful from business perspective, as now companies knows that the first month of new year i.e January is the month for highest number of posts, hence companies can launch and advertise their products during this peak time.


### **3. Posts by weekdays**
```{r}

ggplot(df[,.N,by=post_day],aes(x = post_day,y = N,fill=N,label=round(N,2)))+
  geom_bar(stat = "identity")+labs(title="Posts by day",subtitle="Number of Posts")+xlab("Day")+ylab(NULL)+geom_text(size=5, vjust=1, color="white")

```

### **Observation-**
  * It's interesting to see the highest number of posts that has been posted is on a weekday i.e Tuesday followed by Monday and then Sunday 

# **Top 20 authors**

### **TOP 20 Authors - By number of posts**
```{r}

top_author<-df[,.N,by=author][order(-N)][1:20]
top_author

```


### **TOP 20 Authors - By post score**
```{r}
df[,.("Post_Score"=sum(post_score,na.rm=T)),by=author][order(-Post_Score)][1:20]
```

# **Data Cleaning to get clean text**

```{r}

#Replacing the URLS
# df_data <- df %>% mutate(comment = str_replace_all(comment, "https?://[A-Za-z0-9./]+", ""))

df_data <- df %>% mutate(comment = str_replace_all(comment, "//^(?:http(?:s)?:\\//\\//)?(?:[^\\.]+\\.)?[a-zA-Z0-9]\\.com(\\//.*)?$", ""))


#Replacing HTML tags
df_data <- df_data %>% mutate(comment = str_replace_all(comment, "(<br />)+", ""))


#Remove # in hashtags
df_data <- df_data %>% mutate(comment = str_replace_all(comment, "#([^\\s]+)", "\1"))

#Remove punctuations,numbers and special characters
df_data <- df_data %>% mutate(comment = str_replace_all(comment, "^[a-zA-Z0-9]*$", ""))

```


# **NLP procedure -**
```{r}
get_cleaned_tokens <- function(df_data,redditname) {
  if (redditname == 'all') {
    df_data <- df_data
  } else {
    df_data <- subset(df_data,subreddit == redditname)
  }

  tokens <- df_data %>% unnest_tokens(output = word, input = comment)
  tokens %>%  count(word,sort = TRUE)
  
  #get_stopwords()
  #get stop words
  sw = get_stopwords()
  #cleaned_tokens <- tokens %>%  anti_join(get_stopwords())
  cleaned_tokens <- tokens %>%  filter(!word %in% sw$word)

    nums <- cleaned_tokens %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique()
  #head(nums)
  cleaned_tokens <- cleaned_tokens %>%   anti_join(nums, by = "word")
  #head(cleaned_tokens)

  cleaned_tokens %>%
    count(word, sort = T) %>%
    rename(word_freq = n) %>%
    ggplot(aes(x=word_freq)) +
    geom_histogram(aes(y=..count..), color="black", fill="blue", alpha=0.3) +
    scale_x_continuous(breaks=c(0:5,10,100,500,10e3), trans="log1p", expand=c(0,0)) +
    scale_y_continuous(breaks=c(0,100,1000,5e3,10e3,5e4,10e4,4e4), expand=c(0,0)) +
    theme_bw()
  
  rare <- cleaned_tokens %>%   count(word) %>%  filter(n<10) %>%  select(word) %>% unique()
  head(rare)
  
  rare <- cleaned_tokens %>%
    count(word) %>%
    filter(n<10) %>%
    select(word) %>% unique()

  alpha_remove <- cleaned_tokens %>% filter(str_detect(word, "^[Ã¢|s|t|r|gt|http]$")) %>%   select(word) %>% unique()
  
  
  cleaned_tokens <- cleaned_tokens %>%
    filter(!word %in% rare$word)
  length(unique(cleaned_tokens$word))
  
  cleaned_tokens <- cleaned_tokens %>% filter(!word %in% alpha_remove$word)
  return(cleaned_tokens)
  }
  

```


## **Word cloud for world news**
```{r}

cleaned_tokens = get_cleaned_tokens(df_data,'worldnews')
```

### **define a nice color palette**
```{r}

pal <- brewer.pal(8,"Dark2")
```

### **plot the 100 most common words**
```{r}
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

## **Word cloud for AskReddit**
```{r}

cleaned_tokens = get_cleaned_tokens(df_data,'AskReddit')
```

### **define a nice color palette**
```{r}
pal <- brewer.pal(8,"Dark2")
```

### **plot the 100 most common words**
```{r}
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

### **Word cloud for bangtan**
```{r}
#Word cloud for bangtan
cleaned_tokens = get_cleaned_tokens(df_data,'bangtan')
```

### **Define a nice color palette**
```{r}

pal <- brewer.pal(8,"Dark2")
```

### **Plot the 100 most common words**
```{r}

cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

## **Word cloud for NintendoSwitch**
```{r}
cleaned_tokens = get_cleaned_tokens(df_data,'NintendoSwitch')
```

### **define a nice color palette**
```{r}

pal <- brewer.pal(8,"Dark2")
```

### **plot the 100 most common words**
```{r}

cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

## **Getting word cloud for 'all' so that sentiment analysis will work on everything**
```{r}

cleaned_tokens = get_cleaned_tokens(df_data,'all')
```

### **define a nice color palette**
```{r}

pal <- brewer.pal(8,"Dark2")
```

### **plot the 100 most common words**
```{r}

cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))

```

# **Sentiment analysis -**
```{r}


get_sentiments("nrc")
get_sentiments("afinn")


sent_reviews = cleaned_tokens %>%   
  left_join(get_sentiments("nrc")) %>%  
  rename(nrc = sentiment) %>%  
  left_join(get_sentiments("bing")) %>%  
  rename(bing = sentiment) %>%  
  left_join(get_sentiments("afinn")) %>%  
  rename(afinn = value)
#kable(head(sent_reviews,5))
#head(sent_reviews,5)
```

### **Most common positive and negative words**
```{r}

bing_word_counts <- sent_reviews %>%  
  filter(!is.na(bing)) %>%  
  count(word, bing, sort = TRUE)
#kable(head(bing_word_counts,5))
head(bing_word_counts,5)

```

### **Plotting a graph for most positive and negative words**
```{r}
bing_word_counts %>%  
  filter(n > 700) %>% 
  mutate(n = ifelse(bing == "negative", -n, n)) %>%  
  mutate(word = reorder(word, n)) %>%  
  ggplot(aes(word, n, fill = bing)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")

```



### **Observation-**
  * From the above graph we see that the words like good, like, love, sweet, great are positive words and the words like bitter, dark, nasty, horrible and hate are negative words.


# **Tokenizing by n-grams **

```{r}

df_data$comment<-as.character(df_data$comment)

bigrams <- df_data %>%  unnest_tokens(bigram, comment, token = "ngrams", n = 2)
```

### **Most common bigrams**

```{r}


bigrams %>%  count(bigram, sort = TRUE)
```

### **Filtering by n-grams**

```{r}

bigrams_separated <- bigrams %>%  
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%  
  filter(!word1 %in% stop_words$word) %>%  
  filter(!word2 %in% stop_words$word)
```

### **Bi-gram counts**
```{r}
bigrams_filtered %>%   count(word1, word2, sort = TRUE)


bigram_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigram_counts <- bigram_united %>% 
  count(bigram, sort = TRUE)

bigram_counts=bigram_counts[-1,]

```

# **Visualizing top 10 bigram words-**
```{r}
bigram_counts %>% arrange(desc(n))%>% head(10)%>%ggplot(aes(x=factor(bigram,levels=bigram),y=n))+geom_bar(stat="identity",fill="#003E45")+labs(title="Top 10 bigram words")+coord_flip()

```

# **Document term matrix**
```{r}


word_counts_by_doc_id <- cleaned_tokens %>%  
  group_by(id) %>%  
  count(word, sort = TRUE)

review_dtm <- word_counts_by_doc_id %>%  
  cast_dtm(id, word, n)

review_dtm

```

```{r}
#inspect(review_dtm)
library(topicmodels)
lda8 <- LDA(review_dtm, k = 8, control = list(seed = 1234))
terms(lda8, 20)
```

```{r}
lda8_betas <- broom::tidy(lda8)

lda8_betas

```

```{r}
library(ggrepel)
terms_in_comments <- lda8_betas %>%  
  group_by(topic) %>%  
  top_n(5, beta) %>%  
  ungroup() %>%  
  arrange(topic, -beta)

terms_per_comments <- function(lda_model, num_words) {


  topics_tidy <- tidy(lda_model, matrix = "beta")
  top_terms <- topics_tidy %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  slice(seq_len(num_words)) %>%
  arrange(topic, beta) %>%
  mutate(row = row_number()) %>%
  ungroup() %>%
  mutate(topic = paste("Comment_Topic", topic, sep = " "))
  title <- paste("LDA Top Terms for", k, "Comment_Topics")
  comments_wordchart(top_terms, top_terms$term, title)
}

comments_wordchart <- function(data, input, title) {
  data %>%

  ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +

  geom_point(color = "transparent") +

  geom_label_repel(nudge_x = .2,  
                   direction = "y",
                   box.padding = 0.1,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~topic) +
  theme_comments() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 9),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  labs(x = NULL, y = NULL, title = title) +
    
  ggtitle(title) +
  coord_flip()
}

theme_comments <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}

k <-8
terms_per_comments(lda8,15)

```


# **tf-idf**
```{r}


tfidf <- word_counts_by_doc_id %>%  
  bind_tf_idf(word, id, n) 

#kable(head(tfidf,5))
head(tfidf,5)
```

### Looking at tf-idf we can understand the words that are important for the particular document but not in the corpus

```{r}

top_tfidf = tfidf %>%  
  group_by(id) %>%  
  arrange(desc(tf_idf)) %>%  
  top_n(3) %>% ungroup() %>%  
  arrange(id, -tf_idf)

#kable(head(top_tfidf,5))
head(top_tfidf,5)
```



```{r}
library(dplyr)
df_data %>% 
  select(comment) %>% 
  sentimentr::get_sentences() %>% 
  sentimentr::sentiment() %>% 
  mutate(characters = nchar(stripWhitespace(comment))) %>% 
  filter(characters >1 ) -> bounded_sentences 

summary(bounded_sentences$sentiment)
```


# **Removing positive and negative sentiments**
```{r}
bounded_sentences %>% filter(between(sentiment,-1,1)) ->  bounded_sentences
sentiment_densities <- with(density(bounded_sentences$sentiment), data.frame(x, y))
```


# **Visualizing sentiment densities for the movies**
```{r}
ggplot(sentiment_densities, aes(x = x, y = y)) +
  geom_line() +
  geom_area(mapping = aes(x = ifelse(x >=0 & x<=1 , x, 0)), fill = "green") +
  geom_area(mapping = aes(x = ifelse(x <=0 & x>=-1 , x, 0)), fill = "red") +
  scale_y_continuous(limits = c(0,2.5)) +
  theme_minimal(base_size = 16) +
  labs(x = "Sentiment", 
       y = "", 
       title = "Distribution of Sentiment Across Reddit comments") +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y=element_blank()) -> gg

plot(gg)


```


## **Observations-**
  * In the above graph we can see that the sentiments are fairly distributed


# **NLP procedure summary-**

### Natural Language Processing allows machine to understand the text. We have implemented following in NLP:
  
  1. Tokenization helps us to cut the corpus into pieces known as tokens. For further processing, this tokens list is provided.

  2. Removing stop words is neccesary as the most frequent language words are stop words that must be filtered out in order to get more meaning out of them before data processing.

  3. Removing numbers is done as it is text analysis, I removed numbers because the analysis does not add enough meaning.

  4. Removing rare words is done in a corpus, seldom words usually occur less than 100. They don't add any significance to the corpus, so I took them away.

  5. Sentiment analysis is done To find the positive and negative sentiments of text.

  6. Bigrams are used which is a sequence of 2 adjacent elements of a string of tokens. It is n=2 and are used for text analysis.

  7. Word correlations are used to find the correlated words so as to know the most related words by which we use to remove further uncommon words.

  8. Document Term Matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents.



## **Final remarks**



## **References**

We used the following reference for this assignment. ( each reference is hyperlinked)

* [1] [R markdown](https://monashbioinformaticsplatform.github.io/2017-11-16-open-science-training/topics/rmarkdown.html)
* [2] [R markdown cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf)
* [3] [Data cleaning ](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-23)
* [4] [ggplot2 Visualizations](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Ordered%20Bar%20Chart)


